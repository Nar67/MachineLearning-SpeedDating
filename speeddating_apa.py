# -*- coding: utf-8 -*-
"""SpeedDating-APA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D06XZNg1IZjHaOiULFZX-W82NIfwMlnT

#[SpeedDating](https://www.openml.org/d/40536)

[Download dataset](https://www.openml.org/data/get_csv/13153954/speeddating.arff)
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
from pandas import read_csv
from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import LocalOutlierFactor
from sklearn import preprocessing
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder

from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import LeaveOneOut
from sklearn import svm
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier


from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
import warnings
warnings.filterwarnings('ignore')
import time

"""#Basic Inspection

Substitució del caràcter '?' per *NaNs* i assignació de tipus.
"""

sd = read_csv("speeddating.csv", header=0, delimiter=',')
sd = sd.replace('?', np.nan)
sd = sd.drop(['has_null','wave'], axis= 1)
types = np.empty(len(sd.columns), dtype=object) 
types[0:1] = 'category'
types[1:4] = 'float'
types[4:8] = 'category'
types[8:10] = 'float'
types[10:13] = 'category'
types[13:19] = 'float'
types[19:25] = 'category'
types[25:31] = 'float'
types[31:37] = 'category'
types[37:43] = 'float'
types[43:49] = 'category'
types[49:54] = 'float'
types[54:59] = 'category'
types[59:65] = 'float'
types[65:71] = 'category'
types[71:88] = 'float'
types[88:105] = 'category'
types[105:106] = 'float'
types[106:107] = 'category'
types[107:110] = 'float'
types[110:113] = 'category'
types[113:115] = 'float'
types[115:117] = 'category'
types[117:118] = 'float'
types[118:121] = 'category'
index = 0
sdTypes = sd
for n in sd:
    sdTypes = sdTypes.astype({n:types[index]})
    index += 1
sd = sdTypes

"""#Data Cleaning

Eliminem les columnes amb menys relevancia i les que ja tenim la info per la diferencia
"""

l = [n[2:] for n in [x for x in sd.columns if x[0] == 'd' and x[1] == '_' and 'age' not in x] if n[2:] in sd.columns][:-5]
l.append('field')
l

sd = sd.drop(columns=l)

"""Mirem quines tenen mes NaNs ara"""

per = sd.isna().sum()*100/len(sd)
per.sort_values(ascending=False)

sd = sd.drop(columns=['d_d_age', 'expected_num_interested_in_me', 'expected_num_matches'])

sd = sd.assign(Count_NA = lambda x: x.isnull().sum(axis=1)*100/len(sd.columns)).sort_values('Count_NA', ascending=False)

sd

sd = sd[sd.isnull().sum(axis=1) < 15]
sd = sd.drop(columns=['Count_NA'])

per = sd.isna().sum()*100/len(sd)
per.sort_values(ascending=False)

"""#Data Imputation"""

sd = sd.sort_index()

for col in sd.columns:
  sd[col].fillna(sd[col].mode()[0], inplace=True)
sd = sd.fillna(sd.median())

sd.isna().sum().sort_values(ascending=False)

sd

"""#Outliers

##Age
"""

fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))
sd.boxplot(column='age',ax=axes[0])
sd.hist(column='age',ax=axes[1])

local_outlier_factor = LocalOutlierFactor(n_neighbors=20)
result = local_outlier_factor.fit_predict(sd['age'].values.reshape(-1, 1))

outliers = result == -1 
no_outliers = result == 1
Counter(result)

fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))
sd[no_outliers].boxplot(column='age',ax=axes[0])
sd[no_outliers].hist(column='age',ax=axes[1])
sd = sd[sd.age != 55]

"""##Age_o"""

fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))
sd.boxplot(column='age_o',ax=axes[0])
sd.hist(column='age_o',ax=axes[1])

local_outlier_factor = LocalOutlierFactor(n_neighbors=20)
result = local_outlier_factor.fit_predict(sd['age_o'].values.reshape(-1, 1))

outliers = result == -1 
no_outliers = result == 1
Counter(result)

fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))
sd[no_outliers].boxplot(column='age_o',ax=axes[0])
sd[no_outliers].hist(column='age_o',ax=axes[1])
sd = sd[sd.age_o != 55]

sd.shape

"""#Visualization (pre-normalization)"""

sns.catplot(x='match',y='age',data=sd,palette='rainbow', hue='gender', kind="box")

"""#Normalization, standarization and encoding"""

sd['d_pref_o_sincere'][0][0] == '['

interval_categories = []
other_categories = []
for col in sd.columns:
  if sd[col].dtype != 'float':
    if str(sd[col][0])[0] == '[':
      interval_categories.append(col)
    else:
      other_categories.append(col)

enc = OrdinalEncoder()
enc.fit(sd[interval_categories])
sd[interval_categories] = enc.transform(sd[interval_categories])

sd[interval_categories].describe()

dummies = pd.get_dummies(sd[other_categories])
dummies

dummies = dummies.drop(['gender_male','samerace_0', 'decision_0', 'decision_1', 'decision_o_0', 'decision_o_1', 'match_0'], axis= 1)
dummies.rename(columns = {'samerace_1':'samerace', 'match_1': 'match'}, inplace = True) 
dummies

sd = sd.drop(columns=other_categories)
sd = pd.concat([sd, dummies], axis=1)
sd

min_max_scaler =  preprocessing.MinMaxScaler()
for col in sd:
  sd[[col]] = min_max_scaler.fit_transform(sd[[col]])

#sd.to_csv('speeddating_preproc', index=False)
#sd.describe()

"""#CV"""

#sd = read_csv("speeddating_preprocessed.csv", header=0, delimiter=',')
X = sd.loc[:,sd.columns != 'match']
y = sd['match']

models = [LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(), LogisticRegression(max_iter=400), LinearSVC()]
models_strings = ['LinearDiscriminantAnalysis' , 'QuadraticDiscriminantAnalysis' , 'LogisticRegression', 'LinearSVC']
LDAparams = {
    'solver' : ['svd', 'lsqr'],
    'shrinkage' : [None, 'auto'],
}
QDAparams = {
    'reg_param': [0.1, 0.2, 0.3, 0.4, 0.5, 1],
}

LogRegparams = {
    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],
    'C': [0.1, 0.25, 0.5, 0.75, 1, 1.5, 1.25, 2, 2.5],
    'solver' : ['liblinear', 'sag', 'saga'] 
}

LinearSVCparams = {
    'penalty': ['l1', 'l2'], #nomes hi ha aquestes dos opcions
    'loss': ['hinge', 'squared_hinge'], #nomes hi ha aquestes dos opcions
    'C': [0.1, 0.25, 0.5, 0.75, 1, 1.5, 1.25, 2, 2.5],
}

params = [LDAparams, QDAparams, LogRegparams, LinearSVCparams]

for i, model in enumerate(models):
  print(models_strings[i])
  start = time.time()
  for k in range(2,15):
    start_k = time.time()
    cv = KFold(n_splits=k, shuffle=True, random_state=420)
    gs = GridSearchCV(model,params[i], scoring='accuracy', n_jobs= -1, cv=cv)
    gs.fit(X, y)
    print("\t params: {} || score: {} || k: {} || time: {}".format(gs.best_params_, gs.best_score_, k, time.time()-start_k))
  print("{} took {} seconds".format(models_strings[i], time.time()-start))

"""#Model Evaluation"""

#sd = read_csv("speeddating_preprocessed.csv", header=0, delimiter=',')
X = sd.loc[:,sd.columns != 'match']
y = sd['match']

"""##Linear/Quadratic

###LDA
"""

lda = LinearDiscriminantAnalysis(shrinkage = None, solver = 'lsqr')
kf = KFold(n_splits=6, shuffle=True, random_state=420)
kf.get_n_splits(X)
for train_index, test_index in kf.split(X):
  X_train, X_test = X.iloc[train_index], X.iloc[test_index]
  y_train, y_test = y.iloc[train_index], y.iloc[test_index]

lda.fit(X_train,y_train)
lda_pred = lda.predict(X_test)
r2_lda = lda.score(X_test,y_test)

print('Mean sqared error with validation data: {}'.format(mean_squared_error(y_test,lda_pred)))
print('R2 score with validation data: {}'.format(r2_lda))
print('Coefficients: \n', lda.coef_)

lda_coef_table = pd.DataFrame(list(X_train.columns)).copy()
lda_coef_table.insert(len(lda_coef_table.columns),"Coefs",lda.coef_.transpose())

lda_coef_table.sort_values(by = 'Coefs', ascending=False)

labels = lda_coef_table.sort_values(by=['Coefs']).head(5)
labels = labels.append(lda_coef_table.sort_values(by=['Coefs']).tail(5))
labels = labels.loc[:,0]
labels

mean_coef_table = lda_coef_table.drop(labels.index,axis= 0)
mean_coef_table.sort_values(by = 'Coefs', ascending=False)

np.mean(mean_coef_table.Coefs)

"""####Visualization"""

n = 5

lda_coefs = lda_coef_table.sort_values(by = 'Coefs', ascending=False)
lda_coefs.columns = ['Feature','Coeficients']
plt.xticks(rotation=90)
sns.barplot(x="Feature", y="Coeficients", data=pd.concat([lda_coefs.head(n), lda_coefs.tail(n)])).set_title('Most important features')

"""###Logistic Regression"""

lreg = LogisticRegression(max_iter=400, C = 1, penalty= 'l1', solver = 'liblinear')
kf = KFold(n_splits=13, shuffle=True, random_state=420)
kf.get_n_splits(X)
for train_index, test_index in kf.split(X):
  X_train, X_test = X.iloc[train_index], X.iloc[test_index]
  y_train, y_test = y.iloc[train_index], y.iloc[test_index]

lreg.fit(X_train,y_train)
lreg_pred = lreg.predict(X_test)
r2_lreg = lreg.score(X_test,y_test)

print('Mean sqared error with validation data: {}'.format(mean_squared_error(y_test,lreg_pred)))
print('R2 score with validation data: {}'.format(r2_lreg))
print('Coefficients: \n', lreg.coef_)

lreg_coef_table = pd.DataFrame(list(X_train.columns)).copy()
lreg_coef_table.insert(len(lreg_coef_table.columns),"Coefs",lreg.coef_.transpose())
lreg_coef_table.sort_values(by = 'Coefs', ascending=False)

labels = lreg_coef_table.sort_values(by=['Coefs']).head(5)
labels = labels.append(lreg_coef_table.sort_values(by=['Coefs']).tail(5))
labels = labels.loc[:,0]
labels

mean_coef_table = lreg_coef_table.drop(labels.index,axis= 0)
mean_coef_table.sort_values(by = 'Coefs', ascending=False)
np.mean(mean_coef_table.Coefs)

"""####Visualization"""

logreg_coefs = lreg_coef_table.sort_values(by = 'Coefs', ascending=False)
logreg_coefs.columns = ['Feature','Coeficients']
plt.xticks(rotation=90)
sns.barplot(x="Feature", y="Coeficients", data=pd.concat([logreg_coefs.head(n), logreg_coefs.tail(n)])).set_title('Most important features')

"""###Linear SVM"""

svm = LinearSVC(C = 2, loss = 'squared_hinge', penalty = 'l2')
kf = KFold(n_splits=9, shuffle=True, random_state=420)
kf.get_n_splits(X)
for train_index, test_index in kf.split(X):
  X_train, X_test = X.iloc[train_index], X.iloc[test_index]
  y_train, y_test = y.iloc[train_index], y.iloc[test_index]

svm.fit(X_train,y_train)
svm_pred = svm.predict(X_test)
r2_svm = svm.score(X_test,y_test)

print('Mean sqared error with validation data: {}'.format(mean_squared_error(y_test,svm_pred)))
print('R2 score with validation data: {}'.format(r2_svm))
print('Coefficients: \n', svm.coef_)

svm_coef_table = pd.DataFrame(list(X_train.columns)).copy()
svm_coef_table.insert(len(svm_coef_table.columns),"Coefs",svm.coef_.transpose())
svm_coef_table.sort_values(by = 'Coefs', ascending=False)

labels = svm_coef_table.sort_values(by=['Coefs']).head(5)
labels = labels.append(svm_coef_table.sort_values(by=['Coefs']).tail(5))
labels = labels.loc[:,0]
labels

mean_coef_table = svm_coef_table.drop(labels.index,axis= 0)
mean_coef_table.sort_values(by = 'Coefs', ascending=False)
np.mean(mean_coef_table.Coefs)

"""####Visualization"""

svm_coefs = svm_coef_table.sort_values(by = 'Coefs', ascending=False)
svm_coefs.columns = ['Feature','Coeficients']
plt.xticks(rotation=90)
sns.barplot(x="Feature", y="Coeficients", data=pd.concat([svm_coefs.head(n), svm_coefs.tail(n)])).set_title('Most important features')

"""##Non-Linear"""

X = sd.loc[:,sd.columns != 'match']
y = sd['match']

"""###SVM RBF Kernel"""

C_range = [0.01, 1, 10, 100, 200]
gamma_range = [0.01, 1, 10, 100]
param_grid = {
    "C" : C_range,
    "gamma" : gamma_range,
}

cv = KFold(n_splits=5, shuffle=True, random_state=420)
gs = GridSearchCV(SVC(),param_grid, scoring='accuracy', n_jobs= -1, cv=cv)
gs.fit(X, y)
print("\t params: {} || score: {}".format(gs.best_params_, gs.best_score_))

"""####Evaluation"""

svm_rbf =  SVC(kernel = 'rbf', C = 100,gamma = 0.01, random_state = 420)
kf = KFold(n_splits=5, shuffle=True, random_state=420)
kf.get_n_splits(X)
for train_index, test_index in kf.split(X):
  X_train, X_test = X.iloc[train_index], X.iloc[test_index]
  y_train, y_test = y.iloc[train_index], y.iloc[test_index]

svm_rbf.fit(X_train,y_train)
rbf_pred = svm_rbf.predict(X_test)
r2_rbf = svm_rbf.score(X_test,y_test)
print('Mean sqared error with validation data: {}'.format(mean_squared_error(y_test,rbf_pred)))
print('R2 score with validation data: {}'.format(r2_rbf))

"""###Random Forest"""

params = {
    'n_estimators' : [10, 100, 200, 400, 600, 800, 1000, 1500, 2000],
    'max_depth': [10, 25, 50, 60, 75, 100, None],
    'max_features': ['auto', 'sqrt'],
}
gs = GridSearchCV(RandomForestClassifier(),params, scoring='accuracy', n_jobs= -1)
gs.fit(X, y)
print("\t params: {} || score: {}".format(gs.best_params_, gs.best_score_))

"""####Evaluation"""

rfc = RandomForestClassifier()
kf = KFold(n_splits=5, shuffle=True, random_state=420)
kf.get_n_splits(X)
for train_index, test_index in kf.split(X):
  X_train, X_test = X.iloc[train_index], X.iloc[test_index]
  y_train, y_test = y.iloc[train_index], y.iloc[test_index]

rfc.fit(X_train,y_train)
rfc_pred = rfc.predict(X_test)
r2_rfc = rfc.score(X_test,y_test)

print('Mean sqared error with validation data: {}'.format(mean_squared_error(y_test,rfc_pred)))
print('R2 score with validation data: {}'.format(r2_rfc))
print('Coefficients: \n', rfc.feature_importances_)

rfc_coef_table = pd.DataFrame(list(X_train.columns)).copy()
rfc_coef_table.insert(len(rfc_coef_table.columns),"Coefs",rfc.feature_importances_.transpose())
rfc_coef_table.sort_values(by = 'Coefs', ascending=False)

labels = rfc_coef_table.sort_values(by=['Coefs']).head(5)
labels = labels.append(rfc_coef_table.sort_values(by=['Coefs']).tail(5))
labels = labels.loc[:,0]
labels

mean_coef_table = rfc_coef_table.drop(labels.index,axis= 0)
mean_coef_table.sort_values(by = 'Coefs', ascending=False)
np.mean(mean_coef_table.Coefs)

"""####Visualization"""

rfc_coefs = rfc_coef_table.sort_values(by = 'Coefs', ascending=False)
rfc_coefs.columns = ['Feature','Coeficients']
plt.xticks(rotation=90)
sns.barplot(x="Feature", y="Coeficients", data=pd.concat([rfc_coefs.head(n), rfc_coefs.tail(n)])).set_title('Most important features')